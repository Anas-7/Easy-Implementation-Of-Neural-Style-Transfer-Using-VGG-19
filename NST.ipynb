{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CWs8Zv-O_gCk",
    "outputId": "a27ea58f-6951-4d4b-e12d-2ccd0fbbc188"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "import sys\n",
    "import cv2\n",
    "print ('Running in colab:', 'google.colab' in sys.modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OarK28Iz_gDD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IQyQ1HSi_gDN"
   },
   "outputs": [],
   "source": [
    "def compute_content_cost(a_C, a_G):\n",
    "    n_H, n_W, n_C = a_C.get_shape().as_list()\n",
    "\n",
    "    a_C_unrolled = tf.reshape(a_C,shape=[n_H * n_W,n_C])\n",
    "    a_G_unrolled = tf.reshape(a_G,shape=[n_H * n_W,n_C])\n",
    "    \n",
    "    #compute the cost with tensorflow\n",
    "    #J_content = 0.5*tf.reduce_sum(tf.square(tf.subtract(a_C_unrolled,a_G_unrolled)))\n",
    "    #The multiplying factor is mentioned as 0.5 in original paper but it is found that 1/(4*s) speeds up computation\n",
    "    #where s = n_H * n_W * n_C (product of dimensions)\n",
    "    J_content = tf.reduce_sum(tf.square(tf.subtract(a_C_unrolled,a_G_unrolled)))/(4*n_H*n_W*n_C)\n",
    "    \n",
    "    return J_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOsXS5i4_gDb"
   },
   "outputs": [],
   "source": [
    "def gram_matrix(A):\n",
    "    GA = tf.matmul(A, A,transpose_b=True)\n",
    "    return GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4lZT8sCR_gDo"
   },
   "outputs": [],
   "source": [
    "def compute_layer_style_cost(a_S, a_G):\n",
    "\n",
    "    n_H, n_W, n_C = a_S.get_shape().as_list()\n",
    "  \n",
    "    a_S = tf.transpose(tf.reshape(a_S,shape=[n_H * n_W,n_C]))\n",
    "    a_G = tf.transpose(tf.reshape(a_G,shape=[n_H * n_W,n_C]))\n",
    "\n",
    "    GS = gram_matrix(a_S)\n",
    "    GG = gram_matrix(a_G)\n",
    "\n",
    "    J_style_layer = tf.reduce_sum(tf.square(tf.subtract(GS,GG)))/(4 * (n_C**2) * (n_H*n_W)**2)\n",
    "        \n",
    "    return J_style_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hSQ30P0i_gDy"
   },
   "outputs": [],
   "source": [
    "STYLE_LAYERS = [\n",
    "    ('block1_conv1', 0.5),\n",
    "    ('block2_conv1', 0.5),\n",
    "    ('block3_conv1', 0.5),\n",
    "    ('block4_conv1', 0.5),\n",
    "    (\"block5_conv1\",0.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lWBCvgGHHOEC"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxyObAwbCSN0"
   },
   "outputs": [],
   "source": [
    "#The following manner of calculating content loss produced good results.\n",
    "CONTENT_LAYERS = [\n",
    "    (\"block1_conv1\",0.3),\n",
    "    (\"block3_conv1\",0.5),\n",
    "    (\"block5_conv1\",0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lKxjXwi-_gD_"
   },
   "outputs": [],
   "source": [
    "def compute_style_cost(feature_extractor,input_block,STYLE_LAYERS):\n",
    "    J_style = 0\n",
    "    for layer_name, coeff in STYLE_LAYERS:\n",
    "        #The input_block is a tensor which consists of the combination, content and style images concatenated together\n",
    "        features_style = feature_extractor(input_block)[layer_name]\n",
    "        a_S = features_style[2,:,:,:]\n",
    "        a_G_style = features_style[0,:,:,:]\n",
    "        J_style_layer = compute_layer_style_cost(a_S, a_G_style)\n",
    "        J_style += coeff * J_style_layer\n",
    "\n",
    "    return J_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGU7Hce2_gEy"
   },
   "outputs": [],
   "source": [
    "#We will set the dimensions we want the images to have.\n",
    "#It is highly recommended to try and choose the images whose dimensions match VGG-19 input size 224x224 \n",
    "#Image resizing of of different dimensions may result in unwanted stretching or compression as well as slow computation\n",
    "baseheight = 224\n",
    "basewidth = 224\n",
    "\n",
    "# The setting below is recommended by the authors of VGG paper who trained the dataset on ImageNet and\n",
    "# found this combination of mean-centering to give best results\n",
    "mean_centering_for_vgg_dataset = [103.939,116.779,123.68,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2nJpXQhX_gFE"
   },
   "outputs": [],
   "source": [
    "def resize_normalise_image(im):\n",
    "    #The commented code below is if you are using the Image library for preprocessing.\n",
    "    #Please adjust the subtraction of mean as OpenCV reads in BGR format and Image library reads in RGB format.\n",
    "    \n",
    "    #hpercent = (baseheight / float(im.size[1]))\n",
    "    #basewidth = int((float(im.size[0]) * float(hpercent)))\n",
    "    #im = im.resize((basewidth, baseheight), Image.ANTIALIAS)\n",
    "    \n",
    "    pixels = np.asarray(im)\n",
    "    pixels = pixels.astype('float32')\n",
    "    #If image is grayscale then we need to stack it so that it has 3 channels\n",
    "    if(len(pixels.shape) == 2):\n",
    "      original_features = [pixels,pixels,pixels]\n",
    "      # Stack them into one array\n",
    "      pixels = np.stack(original_features, axis=2)\n",
    "    pixels[:,:,0] -= mean_centering_for_vgg_dataset[0]\n",
    "    pixels[:,:,1] -= mean_centering_for_vgg_dataset[1]\n",
    "    pixels[:,:,2] -= mean_centering_for_vgg_dataset[2]\n",
    "    return np.expand_dims(pixels,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mf4F-qxA_gFM"
   },
   "outputs": [],
   "source": [
    "#Utilise OpenCV to set the size of all images to 224x224\n",
    "\n",
    "#The content image is read as im1\n",
    "im1 = cv2.imread('paris.jpeg',1)\n",
    "#If you want to convert from BGR TO RGB\n",
    "#im1 = im1[...,::-1]\n",
    "im1 = cv2.resize(im1,(basewidth,baseheight))\n",
    "\n",
    "#The style image is read as im2\n",
    "im2 = cv2.imread('pattern.jpg',1)\n",
    "#If you want to convert from BGR TO RGB\n",
    "#im2 = im2[...,::-1]\n",
    "im2 = cv2.resize(im2,(basewidth,baseheight))\n",
    "\n",
    "# If using the Image library use the code below. \n",
    "#Please uncomment the lines in function resize_normalise_image() if you use Image library.\n",
    "# im1 = Image.open('/content/mypic.jpg')\n",
    "# im2 = Image.open('/content/cubes.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZsSNIElM_gFx"
   },
   "outputs": [],
   "source": [
    "content_img = resize_normalise_image(im1)\n",
    "style_img = resize_normalise_image(im2)\n",
    "# Generate a random noise_image\n",
    "noise_ratio = 0.5\n",
    "noise_image = np.random.uniform(-20, 20, (1, basewidth, baseheight, 3)).astype('float32')\n",
    "# Set the input_image to be a weighted average of the content_image and a noise_image\n",
    "new_img = noise_image * noise_ratio + content_img * (1 - noise_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0KOrO8sy_gG3"
   },
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.compat.v1.reset_default_graph()\n",
    "# Start interactive session\n",
    "sess = tf.compat.v1.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KgwU6joy_gHN"
   },
   "outputs": [],
   "source": [
    "content_img = tf.constant(content_img,tf.float32,name='ContentImage')\n",
    "style_img = tf.constant(style_img,tf.float32,name='StyleImage')\n",
    "new_img = tf.Variable(new_img,tf.float32,name='InputImage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BqQH3SN2_gHb"
   },
   "outputs": [],
   "source": [
    "#Load the VGG model. We dont need to include the output layer and hence the size of download is approximately 80MB only.\n",
    "model = VGG19(weights='imagenet',input_shape=(basewidth,baseheight,3),include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FgfR_Ild_gHi"
   },
   "outputs": [],
   "source": [
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "feature_extractor = Model(inputs=model.inputs, outputs=outputs_dict)\n",
    "\n",
    "#We set these values as tensorflow variables if we wish to write code to change the values\n",
    "#over the course of execution of program based on number of iterations.\n",
    "#The metric can be adjusted according to the value of loss achieved after certain number of iterations.\n",
    "alpha = tf.Variable(50.0,tf.float32,name=\"Alpha\")\n",
    "beta = tf.Variable(50.0,tf.float32,name=\"Beta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecrDbHAG_gH_"
   },
   "outputs": [],
   "source": [
    "#This computes the loss over one pass\n",
    "def compute_block_loss(feature_extractor,new_img,content_img,style_img):\n",
    "    #Extract the features for content_layer\n",
    "    input_block = tf.keras.layers.Concatenate(axis = 0)([new_img,content_img,style_img])\n",
    "    J_content = 0\n",
    "    for content_layer,coeff in CONTENT_LAYERS:\n",
    "      content_features = feature_extractor(input_block)[content_layer]\n",
    "      a_G_content = content_features[0,:,:,:]\n",
    "      a_C = content_features[1,:,:,:]\n",
    "      J_content += coeff * compute_content_cost(a_C,a_G_content)\n",
    "    \n",
    "    #Extract features for style layer\n",
    "    J_style = compute_style_cost(feature_extractor,input_block,STYLE_LAYERS)\n",
    "    #The loss below is set for adjusting the distortion of pixels in the generated image.\n",
    "    #The constant value was found by trial and error.\n",
    "    tv_loss = 0.00005*tf.reduce_sum(tf.image.total_variation(input_block[0,:,:,:]))\n",
    "    J_total = alpha * J_content + beta *J_style + tv_loss\n",
    "    \n",
    "    return J_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "qypcr4ry_gIP",
    "outputId": "188fe83e-66a2-497f-9e59-577b8dbd9c80",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The learning rate can be adjusted as per requirement.\n",
    "#If you are varying alpha and/or beta during execution of program it is recommended\n",
    "#to set a low learning rate to prevent distortion.\n",
    "opt = tf.compat.v1.train.AdamOptimizer(learning_rate=2.3,)\n",
    "#Define the function which is to be minimised.\n",
    "cost = compute_block_loss(feature_extractor,new_img,content_img,style_img)\n",
    "#Create a graph which can be accessed using tensorboard. \n",
    "#Open terminal in directory of notebook followed by typing the command \"tensorboard --logdir logs\" without the quotes and open the link given in terminal. (localhost:6006 typically)\n",
    "writer = tf.compat.v1.summary.FileWriter(\"logs\")\n",
    "writer.add_graph(sess.graph)\n",
    "writer.close()\n",
    "#Pass the variable we want to apply gradient descent on, i.e, new_image\n",
    "train_step = opt.minimize(cost,var_list=[new_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTrK70YI_gIY"
   },
   "outputs": [],
   "source": [
    "def run_model(feature_extractor,new_img,content_img,style_img,train_step,num_iterations = 2000):\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    im = None\n",
    "    for i in range(0,num_iterations,1):\n",
    "  \n",
    "        #Compute the gradient descent\n",
    "        sess.run(train_step)\n",
    "        Jt = sess.run(cost)\n",
    "        \n",
    "        print(\"Iteration: \" + str(i) + \" -- Cost:\" + str(Jt))\n",
    "        #The code below is my approach to dynamically adjust the value of alpha and beta every 250 iterations.\n",
    "        #This is done to increase the loss and thereby increase the rate of gradient descent which tends to slow down\n",
    "        #after certain number of iterations thereby slowing down changes in the generated image.\n",
    "#         if((i+1)%250 == 0):\n",
    "#             assign = tf.compat.v1.assign(beta,sess.run(beta)*5)\n",
    "#             sess.run(assign)\n",
    "#             assign = tf.compat.v1.assign(alpha,sess.run(alpha)*3)\n",
    "#             sess.run(assign)\n",
    "#             print(\"New alpha and beta are \" + str(sess.run(alpha)) + \" \" + str(sess.run(beta)))\n",
    "        \n",
    "        #Make sure to re-adjust the dimensions of image by changing it from (1,x,y,3) to (x,y,3) (similar to np.squeeze)\n",
    "        #and the values being strictly between 0 and 255.\n",
    "        generated_image = sess.run(new_img)[0]\n",
    "        generated_image[:,:,0] += mean_centering_for_vgg_dataset[0]\n",
    "        generated_image[:,:,1] += mean_centering_for_vgg_dataset[1]\n",
    "        generated_image[:,:,2] += mean_centering_for_vgg_dataset[2]\n",
    "        generated_image = np.clip(generated_image,0,255)\n",
    "\n",
    "        #Convert the image to RGB format.\n",
    "        generated_image = np.squeeze(generated_image[...,::-1])\n",
    "        im = Image.fromarray(generated_image.astype('uint8'),'RGB')\n",
    "        # Print every 100 iteration.\n",
    "        if (i+1)%100 == 0:\n",
    "          #Create an ouputs folder in directory of notebook prior to running the program\n",
    "          #Can also choose to remove the characters 'outputs/' if want to store in same directory.\n",
    "          im.save(\"outputs/op\" + str(i+1) + \".png\")\n",
    "    \n",
    "    # save last generated image\n",
    "    im.save(\"generated_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dX38iUU2_gI5",
    "outputId": "37f12270-8f24-4716-eca4-2adcaf280a03",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_model(feature_extractor,new_img,content_img,style_img,train_step)\n",
    "#Close the session after execution\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
