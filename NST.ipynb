{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CWs8Zv-O_gCk",
    "outputId": "92201539-40bd-4d39-de54-d0ce557aa938"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in colab: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "import sys\n",
    "print ('Running in colab:', 'google.colab' in sys.modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OarK28Iz_gDD"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IQyQ1HSi_gDN"
   },
   "outputs": [],
   "source": [
    "def compute_content_cost(a_C, a_G):\n",
    "    n_H, n_W, n_C = a_C.get_shape().as_list()\n",
    "    m=1\n",
    "    a_C_unrolled = tf.reshape(a_C,shape=[m,n_H * n_W,n_C])\n",
    "    a_G_unrolled = tf.reshape(a_G,shape=[m,n_H * n_W,n_C])\n",
    "    \n",
    "    # compute the cost with tensorflow\n",
    "    J_content = 1/(4*n_H*n_W*n_C)*tf.reduce_sum(tf.square(tf.subtract(a_C_unrolled,a_G_unrolled)))\n",
    "    \n",
    "    return J_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOsXS5i4_gDb"
   },
   "outputs": [],
   "source": [
    "def gram_matrix(A):\n",
    "    GA = tf.matmul(A,tf.transpose(A))    \n",
    "    return GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4lZT8sCR_gDo"
   },
   "outputs": [],
   "source": [
    "def compute_layer_style_cost(a_S, a_G):\n",
    "\n",
    "    n_H, n_W, n_C = a_S.get_shape().as_list()\n",
    "    m=1\n",
    "    a_S = tf.transpose(tf.reshape(a_S,shape=[n_H * n_W,n_C * m]))\n",
    "    a_G = tf.transpose(tf.reshape(a_G,shape=[n_H * n_W,n_C * m]))\n",
    "\n",
    "    GS = gram_matrix(a_S)\n",
    "    GG = gram_matrix(a_G)\n",
    "\n",
    "    J_style_layer = 1/(4 * (n_C**2) * (n_H*n_W)**2)*tf.reduce_sum(tf.square(tf.subtract(GS,GG)))\n",
    "        \n",
    "    return J_style_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hSQ30P0i_gDy"
   },
   "outputs": [],
   "source": [
    "STYLE_LAYERS = [\n",
    "    ('block1_conv1', 0.2),\n",
    "    ('block2_conv1', 0.2),\n",
    "    ('block3_conv1', 0.2),\n",
    "    ('block4_conv1', 0.2),\n",
    "    (\"block5_conv1\",0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxyObAwbCSN0"
   },
   "outputs": [],
   "source": [
    "content_layer = \"block5_conv2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lKxjXwi-_gD_"
   },
   "outputs": [],
   "source": [
    "def compute_style_cost(feature_extractor,input_block,STYLE_LAYERS):\n",
    "    J_style = 0\n",
    "\n",
    "    for layer_name, coeff in STYLE_LAYERS:\n",
    "        features_style = feature_extractor(input_block)[layer_name]\n",
    "        a_S = features_style[2,:,:,:]\n",
    "        a_G_style = features_style[0,:,:,:]\n",
    "        \n",
    "        J_style_layer = compute_layer_style_cost(a_S, a_G_style)\n",
    "        J_style += coeff * J_style_layer\n",
    "\n",
    "    return J_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHbMeR61_gEe"
   },
   "outputs": [],
   "source": [
    "def total_cost(J_content, J_style, alpha = 20, beta = 50):\n",
    "    J = alpha * J_content + beta * J_style\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGU7Hce2_gEy"
   },
   "outputs": [],
   "source": [
    "#Its compulsory that images of same width and height are used, and at least specify the baseheight if baseheight != basewidth for both images\n",
    "baseheight = 600\n",
    "basewidth = 600\n",
    "\n",
    "# The setting below is recommended by the authors of VGG paper who trained the dataset on ImageNet and\n",
    "# found this combination of mean-centering to give best results\n",
    "mean_centering_for_vgg_dataset = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2nJpXQhX_gFE"
   },
   "outputs": [],
   "source": [
    "def resize_normalise_image(im):\n",
    "    hpercent = (baseheight / float(im.size[1]))\n",
    "    basewidth = int((float(im.size[0]) * float(hpercent)))\n",
    "    im = im.resize((basewidth, baseheight), Image.ANTIALIAS)\n",
    "    pixels = np.asarray(im)\n",
    "    pixels = pixels.astype('float32')\n",
    "    return (np.expand_dims(pixels,axis=0) - mean_centering_for_vgg_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mf4F-qxA_gFM"
   },
   "outputs": [],
   "source": [
    "#If you are using Google Colab then ensure that you set the path correctly\n",
    "#Typically it is \"/content/image_name.jpg\"\n",
    "im1 = Image.open('mypic.jpg')\n",
    "im2 = Image.open('tealpink.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZsSNIElM_gFx"
   },
   "outputs": [],
   "source": [
    "content_img = resize_normalise_image(im1)\n",
    "style_img = resize_normalise_image(im2)\n",
    "# Generate a random noise_image\n",
    "noise_ratio = 0.6\n",
    "noise_image = np.random.uniform(-20, 20, (1, basewidth, baseheight, 3)).astype('float32')\n",
    "# Set the input_image to be a weighted average of the content_image and a noise_image\n",
    "new_img = noise_image * noise_ratio + content_img * (1 - noise_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0KOrO8sy_gG3"
   },
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.compat.v1.reset_default_graph()\n",
    "# Start interactive session\n",
    "sess = tf.compat.v1.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KgwU6joy_gHN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Programs\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "content_img = tf.constant(content_img,tf.float32,name='ContentImage')\n",
    "style_img = tf.constant(style_img,tf.float32,name='StyleImage')\n",
    "new_img = tf.Variable(new_img,tf.float32,name='InputImage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BqQH3SN2_gHb"
   },
   "outputs": [],
   "source": [
    "#Load the VGG model. We dont need to include the output layer and hence the size of download is approximately 80MB only.\n",
    "model = VGG19(weights='imagenet',input_shape=(basewidth,baseheight,3),include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FgfR_Ild_gHi"
   },
   "outputs": [],
   "source": [
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "feature_extractor = Model(inputs=model.inputs, outputs=outputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecrDbHAG_gH_"
   },
   "outputs": [],
   "source": [
    "#This computes the loss over one pass\n",
    "def compute_block_loss(feature_extractor,new_img,content_img,style_img):\n",
    "    #Extract the features for content_layer\n",
    "    input_block = tf.keras.layers.Concatenate(axis = 0)([new_img,content_img,style_img])\n",
    "    content_features = feature_extractor(input_block)[content_layer]\n",
    "    a_G_content = content_features[0,:,:,:]\n",
    "    a_C = content_features[1,:,:,:]\n",
    "    J_content = compute_content_cost(a_C,a_G_content)\n",
    "    #Extract features for style layer\n",
    "    J_style = compute_style_cost(feature_extractor,input_block,STYLE_LAYERS)\n",
    "    J = total_cost(J_content,J_style)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qypcr4ry_gIP",
    "outputId": "a055c44e-7bff-45bd-b2bc-8b7b5eaaacb2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "#You can adjust the learning rate as you wish to.\n",
    "opt = tf.compat.v1.train.AdamOptimizer(learning_rate=1.5)\n",
    "#We define the function we want to minimise\n",
    "cost = compute_block_loss(feature_extractor,new_img,content_img,style_img)\n",
    "#We create a graph which can be accessed using tensorboard. \n",
    "#Open terminal in directory of notebook followed by typing the command \"tensorboard --logdir logs\" without the quotes and open the link given in terminal. (localhost:6006 typically)\n",
    "writer = tf.compat.v1.summary.FileWriter(\"logs\")\n",
    "writer.add_graph(sess.graph)\n",
    "writer.close()\n",
    "#We pass the variable we want to apply gradient descent on, i.e, new_image\n",
    "train_step = opt.minimize(cost,var_list=[new_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTrK70YI_gIY"
   },
   "outputs": [],
   "source": [
    "def run_model(feature_extractor,new_img,content_img,style_img,train_step,num_iterations = 2000):\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    im = None\n",
    "    for i in range(num_iterations):\n",
    "  \n",
    "        #Compute the gradient descent\n",
    "        sess.run(train_step)\n",
    "        Jt= sess.run(cost)\n",
    "        print(\"Iteration: \" + str(i) + \" -- Cost:\" + str(Jt))\n",
    "        #We make sure to re-adjust the dimensions of image by changing it from (1,x,y,3) to (x,y,3) (similar to np.squueze) and then make sure values are between 0 and 255.\n",
    "        generated_image = np.clip((sess.run(new_img) + mean_centering_for_vgg_dataset)[0],0,255)\n",
    "        im = Image.fromarray(generated_image.astype('uint8'),'RGB')\n",
    "        if (Jt < 900):\n",
    "          break\n",
    "        # Print every 100 iteration.\n",
    "        if i%50 == 0:            \n",
    "          im.save(\"output/\" + str(i) + \".jpg\")\n",
    "    \n",
    "    # save last generated image\n",
    "    im.save(\"output/generated_image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dX38iUU2_gI5",
    "outputId": "a4d3281b-95b2-4da0-b3cd-ff9a6c6e0f0c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_model(feature_extractor,new_img,content_img,style_img,train_step)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Do-nYQaj_gJD"
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dm44J85xhTRg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
