{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CWs8Zv-O_gCk",
    "outputId": "fa25f1f4-cfd0-40cf-edc7-f2a6e5f0bd28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in colab: True\n"
     ]
    }
   ],
   "source": [
    "#Assumed that the user will run it on Google Colab, it will run \n",
    "#the same on local machine but please install the prerequisite libraries.\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "import sys\n",
    "import cv2\n",
    "print ('Running in colab:', 'google.colab' in sys.modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OarK28Iz_gDD"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IQyQ1HSi_gDN"
   },
   "outputs": [],
   "source": [
    "def compute_content_cost(a_C, a_G):\n",
    "    n_H, n_W, n_C = a_C.get_shape().as_list()\n",
    "    m=1\n",
    "    a_C_unrolled = tf.reshape(a_C,shape=[m,n_H * n_W,n_C])\n",
    "    a_G_unrolled = tf.reshape(a_G,shape=[m,n_H * n_W,n_C])\n",
    "    \n",
    "    #compute the cost with tensorflow\n",
    "    #J_content = 0.5*tf.reduce_sum(tf.square(tf.subtract(a_C_unrolled,a_G_unrolled)))\n",
    "    #The multiplying factor is mentioned as 0.5 in original paper but it is found that 1/(s) speeds up computation\n",
    "    #where s = n_H * n_W * n_C (product of dimensions)\n",
    "    J_content = 1/(4*n_H*n_W*n_C)*tf.reduce_sum(tf.square(tf.subtract(a_C_unrolled,a_G_unrolled)))\n",
    "    \n",
    "    return J_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOsXS5i4_gDb"
   },
   "outputs": [],
   "source": [
    "def gram_matrix(A):\n",
    "    GA = tf.matmul(A,tf.transpose(A))    \n",
    "    return GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4lZT8sCR_gDo"
   },
   "outputs": [],
   "source": [
    "def compute_layer_style_cost(a_S, a_G):\n",
    "\n",
    "    n_H, n_W, n_C = a_S.get_shape().as_list()\n",
    "    m = 1\n",
    "    a_S = tf.transpose(tf.reshape(a_S,shape=[n_H * n_W,n_C * m]))\n",
    "    a_G = tf.transpose(tf.reshape(a_G,shape=[n_H * n_W,n_C * m]))\n",
    "\n",
    "    GS = gram_matrix(a_S)\n",
    "    GG = gram_matrix(a_G)\n",
    "\n",
    "    J_style_layer = 1/(4 * (n_C**2) * (n_H*n_W)**2)*tf.reduce_sum(tf.square(tf.subtract(GS,GG)))\n",
    "        \n",
    "    return J_style_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hSQ30P0i_gDy"
   },
   "outputs": [],
   "source": [
    "STYLE_LAYERS = [\n",
    "    ('block1_conv1', 0.2),\n",
    "    ('block2_conv1', 0.2),\n",
    "    ('block3_conv1', 0.2),\n",
    "    ('block4_conv1', 0.2),\n",
    "    (\"block5_conv1\",0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxyObAwbCSN0"
   },
   "outputs": [],
   "source": [
    "content_layer = \"block5_conv2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lKxjXwi-_gD_"
   },
   "outputs": [],
   "source": [
    "def compute_style_cost(feature_extractor,input_block,STYLE_LAYERS):\n",
    "    J_style = 0\n",
    "\n",
    "    for layer_name, coeff in STYLE_LAYERS:\n",
    "        features_style = feature_extractor(input_block)[layer_name]\n",
    "        a_S = features_style[2,:,:,:]\n",
    "        a_G_style = features_style[0,:,:,:]\n",
    "        \n",
    "        J_style_layer = compute_layer_style_cost(a_S, a_G_style)\n",
    "        J_style += coeff * J_style_layer\n",
    "\n",
    "    return J_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHbMeR61_gEe"
   },
   "outputs": [],
   "source": [
    "def total_cost(J_content, J_style, alpha = 20, beta = 50):\n",
    "    J = alpha * J_content + beta * J_style\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGU7Hce2_gEy"
   },
   "outputs": [],
   "source": [
    "#We will set the dimensions we want the images to have.\n",
    "#It is highly recommended to try and choose the images whose dimensions match as \n",
    "#resizing of image may result in unwanted stretching or compression.\n",
    "baseheight = 600\n",
    "basewidth = 600\n",
    "\n",
    "# The setting below is recommended by the authors of VGG paper who trained the dataset on ImageNet and\n",
    "# found this combination of mean-centering to give best results\n",
    "mean_centering_for_vgg_dataset = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2nJpXQhX_gFE"
   },
   "outputs": [],
   "source": [
    "def resize_normalise_image(im):\n",
    "    #The commented code below is if you are using the Image library for preprocessing\n",
    "    #hpercent = (baseheight / float(im.size[1]))\n",
    "    #basewidth = int((float(im.size[0]) * float(hpercent)))\n",
    "    #im = im.resize((basewidth, baseheight), Image.ANTIALIAS)\n",
    "    \n",
    "    pixels = np.asarray(im)\n",
    "    pixels = pixels.astype('float32')\n",
    "    #If image is grayscale then we need to stack it so that it has 3 channels\n",
    "    if(len(pixels.shape) == 2):\n",
    "      original_features = [pixels,pixels,pixels]\n",
    "      # Stack them into one array\n",
    "      pixels = np.stack(original_features, axis=2)\n",
    "    return (np.expand_dims(pixels,axis=0) - mean_centering_for_vgg_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mf4F-qxA_gFM"
   },
   "outputs": [],
   "source": [
    "#Please specify the correct path if using it on local machine.\n",
    "\n",
    "#We use OpenCV to set the size of all images to 600x600\n",
    "im1 = cv2.imread('/content/mypic.jpg',1)\n",
    "#To convert from BGR TO RGB\n",
    "im1 = im1[...,::-1]\n",
    "im1 = cv2.resize(im1,(basewidth,baseheight))\n",
    "\n",
    "im2 = cv2.imread('/content/cubes.jpg',1)\n",
    "#To convert from BGR TO RGB\n",
    "im2 = im2[...,::-1]\n",
    "im2 = cv2.resize(im2,(basewidth,baseheight))\n",
    "\n",
    "# If using the Image library use the code below\n",
    "# im1 = Image.open('/content/mypic.jpg')\n",
    "# im2 = Image.open('/content/cubes.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZsSNIElM_gFx"
   },
   "outputs": [],
   "source": [
    "content_img = resize_normalise_image(im1)\n",
    "style_img = resize_normalise_image(im2)\n",
    "# Generate a random noise_image\n",
    "noise_ratio = 0.6\n",
    "noise_image = np.random.uniform(-20, 20, (1, basewidth, baseheight, 3)).astype('float32')\n",
    "# Set the input_image to be a weighted average of the content_image and a noise_image\n",
    "new_img = noise_image * noise_ratio + content_img * (1 - noise_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "0KOrO8sy_gG3",
    "outputId": "d6f6ac29-b34b-433c-e207-0e573b535bf8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1751: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "# Reset the graph\n",
    "tf.compat.v1.reset_default_graph()\n",
    "# Start interactive session\n",
    "sess = tf.compat.v1.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KgwU6joy_gHN"
   },
   "outputs": [],
   "source": [
    "content_img = tf.constant(content_img,tf.float32,name='ContentImage')\n",
    "style_img = tf.constant(style_img,tf.float32,name='StyleImage')\n",
    "new_img = tf.Variable(new_img,tf.float32,name='InputImage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BqQH3SN2_gHb"
   },
   "outputs": [],
   "source": [
    "#Load the VGG model. We dont need to include the output layer and hence the size of download is approximately 80MB only.\n",
    "model = VGG19(weights='imagenet',input_shape=(basewidth,baseheight,3),include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FgfR_Ild_gHi"
   },
   "outputs": [],
   "source": [
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "feature_extractor = Model(inputs=model.inputs, outputs=outputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecrDbHAG_gH_"
   },
   "outputs": [],
   "source": [
    "#This computes the loss over one pass\n",
    "def compute_block_loss(feature_extractor,new_img,content_img,style_img):\n",
    "    #Extract the features for content_layer\n",
    "    input_block = tf.keras.layers.Concatenate(axis = 0)([new_img,content_img,style_img])\n",
    "    content_features = feature_extractor(input_block)[content_layer]\n",
    "    a_G_content = content_features[0,:,:,:]\n",
    "    a_C = content_features[1,:,:,:]\n",
    "    J_content = compute_content_cost(a_C,a_G_content)\n",
    "    #Extract features for style layer\n",
    "    J_style = compute_style_cost(feature_extractor,input_block,STYLE_LAYERS)\n",
    "    J = total_cost(J_content,J_style)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qypcr4ry_gIP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "#The learning rate can be adjusted as per requirement.\n",
    "opt = tf.compat.v1.train.AdamOptimizer(learning_rate=2.5)\n",
    "#Define the function which is to be minimises\n",
    "cost = compute_block_loss(feature_extractor,new_img,content_img,style_img)\n",
    "#Create a graph which can be accessed using tensorboard. \n",
    "#Open terminal in directory of notebook followed by typing the command \"tensorboard --logdir logs\" without the quotes and open the link given in terminal. (localhost:6006 typically)\n",
    "writer = tf.compat.v1.summary.FileWriter(\"logs\")\n",
    "writer.add_graph(sess.graph)\n",
    "writer.close()\n",
    "#Pass the variable we want to apply gradient descent on, i.e, new_image\n",
    "train_step = opt.minimize(cost,var_list=[new_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTrK70YI_gIY"
   },
   "outputs": [],
   "source": [
    "def run_model(feature_extractor,new_img,content_img,style_img,train_step,num_iterations = 2000):\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    im = None\n",
    "    for i in range(num_iterations):\n",
    "  \n",
    "        #Compute the gradient descent\n",
    "        sess.run(train_step)\n",
    "        Jt = sess.run(cost)\n",
    "        print(\"Iteration: \" + str(i) + \" -- Cost:\" + str(Jt))\n",
    "        #Make sure to re-adjust the dimensions of image by changing it from (1,x,y,3) to (x,y,3) (similar to np.squueze) and the values being between 0 and 255.\n",
    "        generated_image = np.clip((sess.run(new_img) + mean_centering_for_vgg_dataset)[0],0,255)\n",
    "        im = Image.fromarray(generated_image.astype('uint8'),'RGB')\n",
    "        if (Jt < 900):\n",
    "          break\n",
    "        # Print every 100 iteration.\n",
    "        if i%50 == 0:            \n",
    "          im.save(\"op\" + str(i) + \".png\")\n",
    "    \n",
    "    # save last generated image\n",
    "    im.save(\"generated_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "id": "dX38iUU2_gI5",
    "outputId": "5ec669a5-6cc0-40a7-c3c4-022c8b2b8471",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 -- Cost:1590268.2\n",
      "Iteration: 1 -- Cost:1534505.1\n",
      "Iteration: 2 -- Cost:1478065.2\n",
      "Iteration: 3 -- Cost:1421301.6\n",
      "Iteration: 4 -- Cost:1364362.1\n",
      "Iteration: 5 -- Cost:1307248.4\n",
      "Iteration: 6 -- Cost:1249912.4\n",
      "Iteration: 7 -- Cost:1192333.5\n",
      "Iteration: 8 -- Cost:1134574.2\n",
      "Iteration: 9 -- Cost:1076776.6\n",
      "Iteration: 10 -- Cost:1019152.44\n",
      "Iteration: 11 -- Cost:961941.3\n",
      "Iteration: 12 -- Cost:905380.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-1b5395d548bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_img\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontent_img\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstyle_img\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-204-d904bfdbce60>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(feature_extractor, new_img, content_img, style_img, train_step, num_iterations)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m#Compute the gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mJt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" -- Cost:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#Make sure to re-adjust the dimensions of image by changing it from (1,x,y,3) to (x,y,3) (similar to np.squueze) and the values being between 0 and 255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 958\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1181\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1182\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_model(feature_extractor,new_img,content_img,style_img,train_step)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Do-nYQaj_gJD"
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wuq86Vh5hvP7"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dm44J85xhTRg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
